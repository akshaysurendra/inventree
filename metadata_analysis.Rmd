---
title: "Metadata analysis"
author: "Krishna Anujan"
date: "07/03/2023"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##What is the status of publishing on tree inventories in India? : An assessment using the novel INvenTree dataset

## Introduction/Background

Global analyses draw on sparse data from the tropics, particularly from South Asian forests. Even though data from India - representing two-thirds of South Asia - exists, a barrier to syntheses is the absence of standardised and accessible data. To address this gap, we formed the India Tree Inventory (INvenTree, https://inventree.weebly.com) Network to harmonise published tree data from forest plots across India into an aggregate inventory and identify geographic and human dimensions of data gaps across biomes in India.

## Objectives

We seek to understand (1) publication biases and (2) geographic gaps in tree community research in India.


```{r reading the files, echo=F}
lvl2<-read.csv("modified_data/InvenTree_WoS-search_2023_level_2_sorting.csv", skip=1)
#metadata$Lat<-as.numeric(metadata$Lat)
#metadata$Long<-as.numeric(metadata$Long)

#removing all the extra empty rows - to avoid having to check the number each time
lvl2[lvl2=="#N/A"]<-NA
lvl2[lvl2==""]<-NA
lvl2<-lvl2[rowSums(is.na(lvl2)) <=ncol(lvl2)-2, ]

#also need to remove any row with Row number in WoS == 3355 for using info from WoS because these are all from a different search

```

```{r summaries, echo=F}

#total number of papers
lvl2_tot<-nrow(lvl2)

#split between number of checklists and plots
num_check<-table(lvl2$Remarks)[1]
num_plot_trans<-table(lvl2$Remarks)[2]+table(lvl2$Remarks)[3]
num_unsure<-table(lvl2$Remarks)[4]

#code for open access Y/N summary

#code for data accessibility Y/N summary

```

## Methods

We created a database of peer-reviewed articles published between 1991-2023 from Indian ecosystems that use tree inventory information from multispecies communities. We used a Web of Science (WoS) Search to search abstracts using broad search terms for tree, vegetation, plot or biomass  based studies in India, which returned 3353 entries and manually sorted these for relevance. With this filtered dataset, we attempted to discern patterns in publication. We used WoS tools to derive publication year, journal names, authors and affiliations and Google maps-based tools to extract locations of author affiliations. We analysed this dataset using community ecology methods, following Mori et al 2015.

```{r trends, echo=F}

yr<-table(lvl2$Pub_Year)[1:31]
plot(yr)

```
```{r functions, echo=F}

library(sp)
library(rworldmap)

#https://stackoverflow.com/questions/21708488/get-country-and-continent-from-longitude-and-latitude-point-in-r

# The single argument to this function, points, is a data.frame in which:
#   - column 1 contains the longitude in degrees
#   - column 2 contains the latitude in degrees
coords2continent = function(points)
{  
  countriesSP <- rworldmap::getMap(resolution='low')
  #countriesSP <- getMap(resolution='high') #you could use high res map from rworldxtra if you were concerned about detail

  # converting points to a SpatialPoints object
  # setting CRS directly to that from rworldmap
  pointsSP = sp::SpatialPoints(points, proj4string=sp::CRS(sp::proj4string(countriesSP)))  


  # use 'over' to get indices of the Polygons object containing each point 
  indices = over(pointsSP, countriesSP)

  #indices$continent   # returns the continent (6 continent model)
  indices$REGION   # returns the continent (7 continent model)
  #indices$ADMIN  #returns country name
  #indices$ISO3 # returns the ISO3 code 
}

coords2country = function(points)
{  
  countriesSP <- rworldmap::getMap(resolution='low')
  #countriesSP <- getMap(resolution='high') #you could use high res map from rworldxtra if you were concerned about detail

  # converting points to a SpatialPoints object
  # setting CRS directly to that from rworldmap
  pointsSP = sp::SpatialPoints(points, proj4string=sp::CRS(proj4string(countriesSP)))  


  # use 'over' to get indices of the Polygons object containing each point 
  indices = over(pointsSP, countriesSP)

  #indices$continent   # returns the continent (6 continent model)
  #indices$REGION   # returns the continent (7 continent model)
  indices$ADMIN  #returns country name
  #indices$ISO3 # returns the ISO3 code 
}

```

```{r addresses pipeline, eval=F, echo=F}

#NEED to avoid repeating the geocoding process - maps has a daily limit

#1. make a lookup table of addresses with lat/long

#files so far
#"modified_data/InvenTree_corresponding_adds_geocoded.csv"
#"modified_data/InvenTree_all_authors_geocoded.csv"

#make sure that all addresses have the full stop from the end removed

add_lat_long<-read.csv("modified_data/InvenTree_corresponding_adds_geocoded.csv")
colnames(add_lat_long)[1]<-"address"
add_lat_long$address <- sub('\\..*', '', add_lat_long$address)

add2<-read.csv("modified_data/InvenTree_all_authors_geocoded.csv")
add2<-add2[,1:3]

add_lat_long<-rbind(add_lat_long, add2)


add3<-read.csv("modified_data/InvenTree_corresponding_adds_12th_March.csv")
colnames(add3)[1]<-"address"
add3<-add3[,1:3]
add3$address <- sub('\\..*', '', add3$address)
add3<-add3[complete.cases(add3),]
add_lat_long<-rbind(add_lat_long, add3)


add4<-read.csv("modified_data/InvenTree_authors_12th_March.csv")
colnames(add4)[1]<-"address"
add4<-add4[,1:3]
add4$address <- sub('\\..*', '', add4$address)
add4<-add4[complete.cases(add4),]
add_lat_long<-rbind(add_lat_long, add4)

add5<-read.csv("modified_data/InvenTree_corresponding_adds_13th_March.csv")
colnames(add5)[1]<-"address"
add5<-add5[,1:3]
add5$address <- sub('\\..*', '', add5$address)
add5<-add5[complete.cases(add5),]
add_lat_long<-rbind(add_lat_long, add5)

add6<-read.csv("modified_data/InvenTree_authors_13th_March.csv")
colnames(add6)[1]<-"address"
add6<-add6[,1:3]
add6$address <- sub('\\..*', '', add6$address)
add6<-add6[complete.cases(add6),]
add_lat_long<-rbind(add_lat_long, add6)

library(plyr)

add_unique<-ddply(add_lat_long, .(address), summarise,
                  lat=mean(Latitude, na.rm=T),
                  long=mean(Longitude, na.rm=T))

write.csv(add_unique, "modified_data/addresses_unique.csv", row.names = F)


```


```{r corresponding authors, echo=F}

add_unique<-read.csv("modified_data/addresses_unique.csv")

#Corresponding author - 1 per paper

cor.locs<-sub(".*), ", "", lvl2$Reprint.address) #extracting only the address and removing corresponding author name
cor.locs<-as.data.frame(cor.locs)

#trial<-tidygeocoder::geocode(cor.locs, cor.locs, method="osm")
#Tried to use R to geocode using Open Street Maps, but these addresses are not in OSM.

cor.locs$Latitude<-rep(NA, nrow(cor.locs))
cor.locs$Longitude<-rep(NA, nrow(cor.locs))
cor.locs$cor.locs2 <- sub('\\..*', '', cor.locs$cor.locs)

# trying to match to unique addresses file

cor.locs[which(cor.locs$cor.locs2%in%add_unique$address),]$Latitude<-add_unique$lat[match(cor.locs[which(cor.locs$cor.locs2%in%add_unique$address),]$cor.locs2, add_unique$address)]

cor.locs[which(cor.locs$cor.locs2%in%add_unique$address),]$Longitude<-add_unique$long[match(cor.locs[which(cor.locs$cor.locs2%in%add_unique$address),]$cor.locs2, add_unique$address)]

#empty rows

cor.locs.empty<-cor.locs[!complete.cases(cor.locs),]


#possibly delete this stuff

#writing the addresses
#write.csv(cor.locs, "modified_data/corresponding_adds.csv", row.names = F)

#exported this and used Awesome Table Geocode

#cor.geo<-read.csv("modified_data/InvenTree_corresponding_adds_geocoded.csv")


```


```{r all authors, echo=F}
#All author addresses

library(stringr)

address.split_vec<-function(paperID, Addresses){
  add5<-as.data.frame(matrix(0, ncol=3))
  colnames(add5)<-c("address", "count", "paperID")
  for(i in 1:length(paperID)){
  add1<-Addresses[i]
  add2<-strsplit(add1, "] ")[[1]] 
add3<-as.data.frame(matrix(0, ncol=2))

add3[1,]<-cbind(strsplit(add2[2], "; ")[[1]][1], str_count(add2[1], ";")+1)
if(length(add2)>2){
  for(i in 3:length(add2)){
    add3<-rbind(add3,cbind(strsplit(add2[i], "; ")[[1]][1], str_count(add2[i-1], ";")))
  }
}
colnames(add3)<-c("address", "count")
add3$paperID<-rep(paperID[i], nrow(add3))
add5<-rbind(add5, add3)
  }
  
return(add5)
}

author_matrix<-address.split_vec(paperID=lvl2$paperID, Addresses=lvl2$Addresses)

#write.csv(author_matrix, "modified_data/author_matrix.csv", row.names=F) #exporting to geocode

# trying to match to unique addresses file

author_matrix$Latitude<-rep(NA, nrow(author_matrix))
author_matrix$Longitude<-rep(NA, nrow(author_matrix))

author_matrix[which(author_matrix$address%in%add_unique$address),]$Latitude<-add_unique$lat[match(author_matrix[which(author_matrix$address%in%add_unique$address),]$address, add_unique$address)]

author_matrix[which(author_matrix$address%in%add_unique$address),]$Longitude<-add_unique$long[match(author_matrix[which(author_matrix$address%in%add_unique$address),]$address, add_unique$address)]

#empty rows

author_matrix.empty<-author_matrix[!complete.cases(author_matrix),]

```
```{r new geocoding, eval=F, echo=F}

write.csv(author_matrix.empty, "modified_data/author_matrix.empty.csv")

write.csv(cor.locs.empty, "modified_data/cor.locs.empty.csv")


```
```{r countries from addresses, echo=F}
#corresponding authors

cor.geo<-cor.locs[complete.cases(cor.locs),]

cor.auth.country<-coords2country(cor.geo[,c(3,2)]) #getting the name of the country

countries<-sort(table(as.character(cor.auth.country)), decreasing=T)
max.cor.country<-names(countries)[1] #country with max
cor.max<-countries[1] #value of max

#all authors

authors_geo<-author_matrix[complete.cases(author_matrix),]
auth.country<-coords2country(authors_geo[,c(5,4)]) #getting the name of the country
authors_geo$country<-as.character(auth.country)

authors_geo$count<-as.numeric(authors_geo$count)
country.freq<-sort(tapply(authors_geo$count, authors_geo$country, sum, na.rm=T), decreasing=T)
```

```{r number of times an author is mentioned}
```

```{r rejected code, echo=F}
address.split<-function(paperID, Addresses){
add1<-Addresses
add2<-strsplit(add1, "] ")[[1]] 

add3<-as.data.frame(matrix(0, ncol=2))
add3[1,]<-cbind(strsplit(add2[2], "; ")[[1]][1], str_count(add2[1], ";")+1)
if(length(add2)>2){
  for(i in 3:length(add2)){
    add3<-rbind(add3,cbind(strsplit(add2[i], "; ")[[1]][1], str_count(add2[i-1], ";")))
  }
}
colnames(add3)<-c("address", "count")
add3$paperID<-rep(paperID, nrow(add3))

return(add3)
}

```

```{r making the maps, echo=F, eval=F}
library(rnaturalearth)
library(rnaturalearthdata)
library(sf)
library(rgeos)
library(ggplot2)
library(sp)
library(rgdal)
library(maptools)

#https://sadique.io/blog/2014/08/08/disputed-territories-and-merging-shapes-and-features/

#world <- ne_countries(scale = "medium", returnclass = "sf")

world_shp<-readOGR("C://Users/krish/Documents/Columbia/Naeem_Lab/Phd_thesis/Chap3/maps/world_borders", "TM_WORLD_BORDERS_SIMPL-0.3")
world_shp2<-unionSpatialPolygons(world_shp, IDs=world_shp@data$NAME)

world_shp<-readOGR("C://Users/krish/Documents/INvenTree/large_data/shapefiles/ne_50m_admin_0_countries", "ne_50m_admin_0_countries")
world<-st_as_sf(world_shp, crs=projcrs)

disputed<-readOGR("C://Users/krish/Documents/INvenTree/large_data/shapefiles/ne_50m_admin_0_boundary_lines_disputed_areas", "ne_50m_admin_0_boundary_lines_disputed_areas")
disputed<-st_as_sf(disputed, crs=projcrs)

projcrs <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
df <-sf::st_as_sf(x = cor.geo,                         
           coords = c("Longitude", "Latitude"),
           crs = projcrs)


png("plots/corr_author_map_12_March.png", width=12, height=8, units="in", res=300)
ggplot(world) +
geom_sf(disputed) +
    geom_sf(color="black")+
  geom_sf(data=df, col=seqinr::col2alpha("forestgreen", 0.5))+
  xlab("Longitude")+ylab("Latitude")+coord_sf(expand = FALSE) +
  scale_y_continuous(limits = c(-79, 79))+theme_minimal()+ggtitle("Corresponding author locations")
dev.off()



```

```{r journal summaries, echo=F}
journals<-sort(table(lvl2$Source.Title), decreasing=T)

#richness of journals

#diversity of journals
pi<-journals/sum(journals)
shannon.journal<-round(-sum(pi*log(pi)), 2)

#richness of corresponding author affiliations
cor.affs<-sort(table(cor.locs$cor.locs), decreasing=T)

#diversity of corresponding author affiliations
pi<-cor.affs/sum(cor.affs)
shannon.cors<-round(-sum(pi*log(pi)), 2)
```

```{r bibliometrix, echo=F}
#bibliometrix::biblioshiny()
```

## Results

Manual sorting resulted in `r lvl2_tot` potentially relevant publications that report on tree inventories from multispecies communities in India either using plot/transect methods (n=`r num_plot_trans`), checklists (n=`r num_check`).

Preliminary results show that publications spanned `r sum(journals)` journals; the top 3 were `r names(journals)[1]` (n=`r journals[1]`), `r names(journals)[2]` (n=`r journals[2]`) and `r names(journals)[3]` (n=`r journals[3]`). The Shannon's diversity of journals was `r shannon.journal`, suggesting high dominance. Across the papers, there were `r sum(authors_geo$count)` instances of authorship with the most number (n=`r country.freq[1]`) from `r names(country.freq)[1]`, followed by `r names(country.freq)[2]` (n=`r country.freq[2]`). Corresponding authors were predominantly affiliated to institutions in `r max.cor.country` (n=`r cor.max` out of `r lvl2_tot`), followed by `r names(countries)[2]` (n=`r countries[2]`). There were a total of `r sum(cor.affs)` corresponding author affiliations with a Shannon's diversity of `r shannon.cors`.

Ongoing analyses will assess geographic and habitat biases.

## Implications

Based on our metadata analysis, we identify opportunities for collaboration and data sharing among Indian scientists as well as between Indian scientists and foreign collaborators towards a more diverse and equitable community of ecologists. We thus motivate the INvenTree network to share and synthesise tree-based data that will help fill crucial gaps in our understanding of forest dynamics from the region and allow novel syntheses and application.


